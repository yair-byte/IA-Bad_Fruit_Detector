# -*- coding: utf-8 -*-
"""Sistema_tiempo_real_modelo.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zeduMguTxZH1MKRVDuhjfHsqXx3buaTG
"""

#LIBRERIAS NECESARIAS
import tensorflow as tf
from tensorflow.keras.preprocessing.image import load_img, img_to_array
import numpy as np
import os
from google.colab import drive 
import cv2
import random
import pandas as pd
from PIL import Image, ImageFilter, ImageEnhance
import matplotlib.pyplot as plt
from tensorflow.keras import layers
from tensorflow.keras import regularizers
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn.metrics import ConfusionMatrixDisplay
from sklearn.metrics import confusion_matrix

drive.mount('/content/gdrive', force_remount=True)

def noisy(noise_typ,image):
  if noise_typ == "gauss":
      row,col,ch= image.shape
      mean = 0

      var = 0.001 #0.001 good

      
      sigma = var**0.5
      gauss = np.random.normal(mean,sigma,(row,col,ch))
      gauss = gauss.reshape(row,col,ch)
      noisy = image + gauss
      return noisy
  elif noise_typ == "poisson":
      vals = len(np.unique(image))
      vals = 2 ** np.ceil(np.log2(vals))
      noisy = np.random.poisson(image * vals) / float(vals)
      return noisy
  elif noise_typ =="speckle":
      row,col,ch = image.shape
      gauss = np.random.randn(row,col,ch)
      gauss = gauss.reshape(row,col,ch)        
      noisy = image + image * gauss
      return noisy
  elif noise_typ == "s&p":
      row,col,ch = image.shape
      s_vs_p = 0.5
      amount = 0.004
      out = np.copy(image)
      # Salt mode
      num_salt = np.ceil(amount * image.size * s_vs_p)
      coords = [np.random.randint(0, i - 1, int(num_salt))
              for i in image.shape]
      out[coords] = 1

      # Pepper mode
      num_pepper = np.ceil(amount* image.size * (1. - s_vs_p))
      coords = [np.random.randint(0, i - 1, int(num_pepper))
              for i in image.shape]
      out[coords] = 0
      return out

data = []
labels = []
k_ant=0

cont=0



path1='/content/gdrive/MyDrive/naranjas'

path2='/content/gdrive/MyDrive/manzanas'

path3='/content/gdrive/MyDrive/bananas'

lista_paths = []
lista_paths.append(path1)
lista_paths.append(path2)
lista_paths.append(path3)


for index_path in range(len(lista_paths)):
  for dirname, _, filenames in os.walk(lista_paths[index_path]):
      for filename in filenames:

          print(cont)
          cont=cont+1

          imgo = load_img(os.path.join(dirname, filename))
          
          #imgo = Image.open(os.path.join(dirname, filename))
          # Creating object of Sharpness class
          

          #k=0

          k=random.randint(0, 6)

          for i in range(1):



            if (k==0):
                img = imgo.transpose(Image.FLIP_LEFT_RIGHT)
                
            elif (k==1):
                img = imgo.transpose(Image.FLIP_TOP_BOTTOM)
            elif (k==2):
                img = imgo.transpose(Image.FLIP_LEFT_RIGHT)
                img = imgo.transpose(Image.FLIP_TOP_BOTTOM)
            elif (k==3):
                img = imgo.transpose(Image.ROTATE_90)
            elif (k==4):
                img = imgo.transpose(Image.ROTATE_270)
            else:
                img = imgo

            img_o = img.filter(ImageFilter.MedianFilter())

            img_o=img

            #brillo = 0.85
            #sharp = 0.01
            #color = 0.5
            zoom = random.uniform(0.97, 0.99)
            for l in range(1):

              brillo = random.uniform(0.95, 1.05)
              sharp = random.uniform(0.95, 1.05)
              color = random.uniform(0.95, 1.05)
              
              enhancer = ImageEnhance.Brightness(img_o) #0.9 a 1
              img2 = enhancer.enhance(brillo)
              enhancer2 = ImageEnhance.Sharpness(img_o)  #0.1 a 2
              img3 = enhancer2.enhance(sharp)
              enhancer3 = ImageEnhance.Color(img_o)  #0.5 a 1
              img4 = enhancer3.enhance(color)


              #brillo = brillo + 0.04
              #sharp = sharp + 0.4
              #color = color + 0.1
            
            
              img4 = img_to_array(img4)
              #img4=tf.image.central_crop(img4,zoom)
           
              img5 = np.array(tf.image.resize(img4, [96,96]))
              
              #img = tf.image.rgb_to_grayscale(img)

              data.append(img5)
              labels.append(int(dirname[-1]))



            #k=k+1

data = np.array(data)
labels = np.array(labels)

print("data.shape: ",data.shape , type(data))
print("labels.shape: ",labels.shape, type(labels))

train_images, test_images, train_labels, test_labels = train_test_split(data , labels, test_size=0.25, shuffle=True)

print(train_images.shape)
print(test_images.shape)

num_classes = 6
canales = 3

dim0 = train_images.shape[0]
dim1 = train_images.shape[1]
dim2 = train_images.shape[2]
dim0_0 = test_images.shape[0]

train_images = train_images.reshape((dim0, dim1, dim2, canales))
test_images = test_images.reshape((dim0_0, dim1, dim2, canales))

# Normalize pixel values to be between -1 and 1
train_images, test_images = (train_images / 127.5) -1 , (test_images / 127.5) -1
#no usar one-hot
#train_labels = tf.keras.utils.to_categorical(train_labels, num_classes)
#test_labels = tf.keras.utils.to_categorical(test_labels, num_classes)

#a√±adir RUIDOOOOOOOOOOOO

for i in range(train_images.shape[0]):
  train_images[i]=noisy("gauss",train_images[i])

'''
for i in range(test_images.shape[0]):
  test_images[i]=noisy("gauss",test_images[i],1)
'''

print(train_images.shape)
print(test_images.shape)

print(train_labels.shape)
print(test_labels.shape)

img_n = 760
img2 = (train_images[img_n] + 1 ) / 2

#img2=img2.reshape((test_images.shape[1],test_images.shape[2]))
#plt.imshow(img2,cmap="gray")


plt.imshow(img2)

print(train_labels[img_n])

model = tf.keras.models.Sequential()


# In the first layer we only quantize the weights and not the input
model.add(tf.keras.layers.Conv2D(8, (3, 3),
                                kernel_regularizer=regularizers.l1_l2(l1=1e-4, l2=1e-4),
                                bias_regularizer=regularizers.l2(1e-4),
                                activation="relu",
                                input_shape=(dim1, dim2, canales)))
model.add(tf.keras.layers.MaxPooling2D((3, 3)))
model.add(tf.keras.layers.BatchNormalization())


model.add(tf.keras.layers.Conv2D(15, (3, 3),activation="relu"))
model.add(tf.keras.layers.MaxPooling2D((3, 3)))
model.add(tf.keras.layers.BatchNormalization())


model.add(tf.keras.layers.Conv2D(25, (3, 3),activation="relu"))
model.add(tf.keras.layers.MaxPooling2D((3, 3)))
model.add(tf.keras.layers.BatchNormalization())


model.add(tf.keras.layers.Flatten())

model.add(tf.keras.layers.Dropout(0.5))

model.add(tf.keras.layers.Dense(20,activation="relu"))
model.add(tf.keras.layers.BatchNormalization())


model.add(tf.keras.layers.Dense(num_classes,activation="softmax"))

model.summary()

callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=30)

opt = tf.keras.optimizers.Adam(learning_rate=0.00001)

model.compile(optimizer=opt,
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
trained_model = model.fit(train_images, train_labels, validation_data=(test_images, test_labels),batch_size=8, epochs=10000,shuffle=True,callbacks=[callback])

plt.plot(trained_model.history['accuracy'])
plt.plot(trained_model.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')

print(np.max(trained_model.history['accuracy']))
print(np.max(trained_model.history['val_accuracy']))

plt.plot(trained_model.history['loss'])
plt.plot(trained_model.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')

print(np.min(trained_model.history['loss']))
print(np.min(trained_model.history['val_loss']))

test_loss, test_acc = model.evaluate(test_images, test_labels)
print(f"Test accuracy {test_acc * 100:.2f} %")

prediccion_test = model.predict(test_images)
y_pred_test = np.argmax(prediccion_test, axis=1)
#test_labels_2 = np.argmax(test_labels, axis=1)

#lista_nombres= list(dicc.keys())
labels_names = ["NARANJAS MALAS","NARANJAS BUENAS","MANZANAS MALAS","MANZANAS BUENAS","BANANAS MALAS","BANANAS BUENAS"]
cm = confusion_matrix(test_labels, y_pred_test)

disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels_names)

disp.plot(cmap=plt.cm.Blues)
plt.show()

model.save("modelo_multiclase_str_96x96x3_final.h5")  # save binary weights
weights = model.get_weights()  # get binary weights